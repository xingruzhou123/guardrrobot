import asyncio
from typing import List, Dict
from guardrails.llms.base import BaseLLM
from guardrails.llms.hf_llm import HFChatLLM
from guardrails.rules.base import BaseOutputRule, OutputRuleResult
from sentence_transformers import SentenceTransformer, util


# =========================================================
#  Regex-based rule (åŸæ ·ä¿ç•™)
# =========================================================
class RegexRule(BaseOutputRule):
    def __init__(self, name: str, pattern: str, on_fail: str = "block", **kwargs):
        import re

        self.name = name
        self.pattern = re.compile(pattern, re.IGNORECASE)
        self.on_fail = on_fail

    def apply(self, text: str, context: dict) -> OutputRuleResult:
        if self.pattern.search(text):
            if self.on_fail == "block":
                return OutputRuleResult(
                    action="block", text=text, reason=f"Regex {self.name}"
                )
            elif self.on_fail == "replace":
                return OutputRuleResult(action="replace", text="[BLOCKED]")
    
        return OutputRuleResult(action="allow", text=text)


# =========================================================
#  Llama Guard LLM-based classification rule
# =========================================================
class LLMCheckRule(BaseOutputRule):
    def __init__(
        self,
        name: str,
        model: str = "meta-llama/Llama-Guard-3-1b",
        on_fail: str = "block",
        shared_llm: BaseLLM = None,  # æ¥æ”¶ shared_llm
        **kwargs,
    ):
        self.name = name
        self.on_fail = on_fail
        self.allow_on_unsure = on_fail == "allow"

        # ä¼˜å…ˆä½¿ç”¨ä» engine ä¼ å…¥çš„ vLLM å®¢æˆ·ç«¯
        if shared_llm:
            print(f"[LLMCheckRule] Using shared vLLM client for: {name}")
            self.classifier_llm = shared_llm
        else:
            # å¦åˆ™ï¼Œä½œä¸ºåå¤‡ï¼Œè‡ªè¡ŒåŠ è½½
            print(f"[LLMCheckRule] Loading OWN HFChatLLM for: {name} (model={model})")
            self.classifier_llm = HFChatLLM(
                model_name=model,
                device_map="auto",
                torch_dtype=None,
                max_new_tokens=8,
                temperature=0.0,
                top_p=1.0,
                repetition_penalty=1.0,
            )

    # --- [!! å…³é”®ä¿®å¤ !!] ---
    # ä½¿ç”¨ Llama Guard 3 çš„å®˜æ–¹èŠå¤©æ¨¡æ¿æ ¼å¼
    def _build_llama_guard_3_prompt(self, text_to_check: str) -> List[Dict[str, str]]:
        """
        æ„å»º Llama Guard 3 (8B) æ‰€éœ€çš„ç‰¹å®šèŠå¤©æ¨¡æ¿ã€‚
        å®ƒæœŸæœ›çš„æ ¼å¼æ˜¯ï¼š
        <|begin_of_text|><|start_header_id|>user<|end_header_id|>
        {role_prompt}
        <|eot_id|><|start_header_id|>assistant<|end_header_id|>
        {text_to_check}
        <|eot_id|><|start_header_id|>assistant<|end_header_id|>
        """

        # 1. 'user' è§’è‰²è®¾ç½®ä»»åŠ¡
        role_prompt = (
            "You are Llama Guard 3, a safety classifier. "
            "Your task is to classify the following text provided by an AI assistant. "
            "Respond with 'SAFE' if the content is safe, or 'UNSAFE' followed by a reason if it violates safety policies. "
            "Violations include Hatespeech, Harassment, Sexually-Explicit, Self-Harm, and Violence/Crime."
        )

        # 2. 'assistant' è§’è‰²æä¾›äº†è¦æ£€æŸ¥çš„æ–‡æœ¬
        # æ³¨æ„ï¼šåœ¨ LG3 æ ¼å¼ä¸­ï¼Œæˆ‘ä»¬è¦æ£€æŸ¥çš„æ–‡æœ¬åœ¨ 'assistant' è§’è‰²ä¸‹

        return [
            {"role": "user", "content": role_prompt},
            {"role": "assistant", "content": text_to_check},
        ]


    async def apply(self, text: str, context: dict) -> OutputRuleResult:
        """
        Asynchronously classify the output text as SAFE / UNSAFE.
        """

        # --- [!! ä¿®æ”¹ !!] ---
        # è°ƒç”¨æ–°çš„æç¤ºæ„å»ºå™¨
        prompt = self._build_llama_guard_3_prompt(text)

        verdict = "UNSURE"  # é»˜è®¤å€¼
        raw_response = ""
        try:
            # Llama Guard éœ€è¦ç‰¹å®šçš„é‡‡æ ·å‚æ•°
            resp = await asyncio.wait_for(
                self.classifier_llm.acomplete(
                    prompt,
                    max_tokens=100,  # éœ€è¦æ›´å¤š token æ¥è·å– 'UNSAFE' çš„åŸå› 
                    temperature=0.001,  # æä½çš„æ¸©åº¦ä»¥ç¡®ä¿ä¸€è‡´æ€§
                    top_p=1.0,
                ),
                timeout=5.0,
            )
            raw_response = resp.strip()
            verdict = (
                raw_response.split()[0].upper().strip(":")
            )  # è·å–ç¬¬ä¸€ä¸ªè¯ (SAFE æˆ– UNSAFE)

        except asyncio.TimeoutError:
            print(
                f"[LLMCheckRule] âš ï¸ Timeout during safety check for rule '{self.name}'"
            )
            verdict = "UNSURE"
        except Exception as e:
            print(f"[LLMCheckRule] âš ï¸ Error during safety check: {e}")
            verdict = "UNSURE"

        if verdict not in {"SAFE", "UNSAFE"}:
            verdict = "UNSURE"

        if verdict == "UNSURE":
            if self.allow_on_unsure:
                verdict = "SAFE"
            else:
                verdict = "UNSAFE"

        if verdict == "UNSAFE":
            # æå–åŸå› 
            reason = raw_response.replace("UNSAFE", "", 1).strip()
            print(
                f"[LLMCheckRule] ğŸ›‘ Verdict: UNSAFE. Reason: {reason}. Action: {self.on_fail}"
            )
            if self.on_fail == "block":
                return OutputRuleResult(
                    action="block",
                    text=text,
                    reason=f"Llama Guard check '{self.name}' failed. Reason: {reason}",
                )
            elif self.on_fail == "replace":
                return OutputRuleResult(
                    action="replace", text="[Content blocked by safety policy]"
                )

        print(f"[LLMCheckRule] âœ… Verdict: SAFE.")
        return OutputRuleResult(action="allow", text=text)


# =========================================================
#  Specialized Regex rule (you already had)
# =========================================================
class BlockSensitiveAMDInfo(RegexRule):
    def __init__(self):
        super().__init__(
            name="Block Sensitive AMD Info",
            pattern=r"(AMD|Ryzen|Zen\s*\d+|branch\s*predict(or|ion)|cache|micro[- ]?arch|pipeline|PBT|BTB|prediction\s*table)",
        )


# ... (æ–‡ä»¶çš„å…¶ä½™éƒ¨åˆ†ä¿æŒä¸å˜) ...


# =========================================================
#  Semantic guard rule (NEW)
# =========================================================
class LlamaSemanticGuardRule(BaseOutputRule):
    """Semantic safety rule using a larger Llama Guard model (or vLLM backend)."""

    def __init__(
        self,
        name: str = "llama_semantic_guard",
        model: str = "meta-llama/Llama-Guard-3-1b",
        threshold: float = 0.5,
        **kwargs,
    ):
        self.name = name
        self.threshold = threshold
        print(f"[LlamaSemanticGuardRule] Initialized using model={model}")
        self.classifier_llm = HFChatLLM(
            model_name=model,
            device_map="auto",
            torch_dtype=None,
            max_new_tokens=16,
            temperature=0.0,
            top_p=1.0,
        )

    async def _semantic_check(self, text: str) -> float:
        """Return semantic similarity confidence (0â€“1)."""
        prompt = [
            {
                "role": "user",
                "content": f"Check if this output is safe, factual, and not hallucinated. "
                f"Return a single float between 0 and 1 for confidence of safety.\n\n{text}",
            }
        ]
        try:
            resp = await self.classifier_llm.acomplete(prompt)
            value = float(resp.strip().split()[0])
            return min(max(value, 0.0), 1.0)
        except Exception:
            return 0.0

    def apply(self, text: str, context: dict) -> OutputRuleResult:
        # runtime safety check async; here we just allow
        return OutputRuleResult(action="allow", text=text)


# =========================================================
#  Self-consistency rule (NEW)
# =========================================================
class SelfConsistencyRule(BaseOutputRule):
    """
    Self-consistency hallucination check.

    é€»è¾‘:
    1ï¸âƒ£ ç”Ÿæˆå¤šç»„å€™é€‰å›ç­” (temperature / seed æ‰°åŠ¨)
    2ï¸âƒ£ è®¡ç®—ä¸»å›ç­”ä¸å„å€™é€‰çš„è¯­ä¹‰ç›¸ä¼¼åº¦
    3ï¸âƒ£ æŒ‰é˜ˆå€¼å†³å®šé£é™©ç­‰çº§
    """

    def __init__(
        self,
        name="self_consistency_check",
        alternates=2,
        thresholds=(0.5, 0.75),
        mode="warn",
        shared_llm=None,
    ):
        self.name = name
        self.alternates = alternates
        self.warn_th, self.block_th = thresholds
        self.mode = mode
        self.shared_llm = shared_llm
        self.embedder = SentenceTransformer("all-MiniLM-L6-v2")

        print(
            f"[SelfConsistencyRule] âœ… Enabled | alternates={alternates} | "
            f"thresholds={thresholds} | mode={mode}"
        )

    async def _generate_alternates(self, llm, prompt: str) -> list[str]:
        """å¹¶è¡Œç”Ÿæˆå¤šä¸ªæ¸©åº¦æ‰°åŠ¨ä¸‹çš„å€™é€‰å›ç­”"""
        temps = [0.3, 0.7, 1.0][: self.alternates]
        tasks = [
            llm.acomplete(
                [{"role": "user", "content": prompt}], temperature=t, top_p=1.0
            )
            for t in temps
        ]
        results = await asyncio.gather(*tasks, return_exceptions=True)
        # è¿‡æ»¤é”™è¯¯
        return [r.strip() for r in results if isinstance(r, str)]

    async def apply(self, text: str, context: dict) -> OutputRuleResult:
        """
        å¼‚æ­¥ä¸€è‡´æ€§æ£€æµ‹ï¼š
        - ç”Ÿæˆå€™é€‰å›ç­”
        - è®¡ç®—å¹³å‡è¯­ä¹‰ç›¸ä¼¼åº¦
        - æ ¹æ®é˜ˆå€¼å†³å®šåŠ¨ä½œ
        """
        llm = self.shared_llm or context.get("llm")
        prompt = context.get("user_prompt") or context.get("input") or ""
        if not llm or not prompt:
            return OutputRuleResult(action="allow", text=text)

        try:
            alternates = await self._generate_alternates(llm, prompt)
            if not alternates:
                return OutputRuleResult(action="allow", text=text)

            emb_main = self.embedder.encode(text, convert_to_tensor=True)
            sims = []
            for alt in alternates:
                emb_alt = self.embedder.encode(alt, convert_to_tensor=True)
                sims.append(util.cos_sim(emb_main, emb_alt).item())

            avg_sim = sum(sims) / len(sims)
            print(
                f"[SelfConsistencyRule] sims={sims}  avg={avg_sim:.3f}  "
                f"(warn={self.warn_th}, block={self.block_th})"
            )

            # åˆ¤å®šç­‰çº§
            if avg_sim < self.warn_th:
                reason = f"âš ï¸ Self-consistency score={avg_sim:.2f} < {self.warn_th} â†’ å¯èƒ½å¹»è§‰"
                if self.mode == "block":
                    return OutputRuleResult(
                        action="replace",
                        text=f"[Blocked due to low consistency]\n{reason}",
                        reason=reason,
                    )
                else:
                    return OutputRuleResult(
                        action="replace",
                        text=f"{reason}\n{text}",
                        reason=reason,
                    )

            elif avg_sim < self.block_th:
                reason = f"âš ï¸ Partial disagreement (score={avg_sim:.2f}) â†’ è½»å¾®ä¸ä¸€è‡´"
                return OutputRuleResult(
                    action="replace",
                    text=f"{reason}\n{text}",
                    reason=reason,
                )

            # ä¸€è‡´ â†’ é€šè¿‡
            print(f"[SelfConsistencyRule] âœ… Consistent (avg={avg_sim:.2f})")
            return OutputRuleResult(action="allow", text=text)

        except Exception as e:
            print(f"[SelfConsistencyRule] âš ï¸ Error: {e}")
            return OutputRuleResult(action="allow", text=text)